@article{Hoffmann2016,
abstract = {The music recommendation systems, currently in use rely upon ratings, likes and generic generalization of genres. This produces far from ideal recommendations and possibly constricts the exploration span of the listener, thus limiting one's library to a set of popular artists and titles. In order to refine the recommendation system, scientific attributes of the track must be taken into account. These attributes can be represented in the form of vector parameters. These vector parameters can be meaningfully defined using a specially designed FFT algorithm and the derived data is sent to the main server which will serve as an open ended system. Thus resulting in a system which leads to an indirect recommendation performed by another user client.},
author = {Hoffmann, Piotr and Kaczmarek, Andrzej and Spaleniak, Pawe{\l} and Kostek, Bo{\.{z}}ena},
doi = {10.3923/ajit.2016.4250.4254},
file = {:home/lucas/work/mendeley/2016/Music Recommendation System{\_}Hoffmann et al.pdf:pdf},
issn = {19935994},
journal = {Asian Journal of Information Technology},
keywords = {Exploration span,Library,Listener,Music recommendation system,Vector parameters},
number = {21},
pages = {4250--4254},
title = {{Music Recommendation System}},
volume = {15},
year = {2016}
}
@article{Gemmeke2017,
abstract = {Audio event recognition, the human-like ability to identify and relate sounds from audio, is a nascent problem in machine perception. Comparable problems such as object detection in images have reaped enormous benefits from comprehensive datasets - principally ImageNet. This paper describes the creation of Audio Set, a large-scale dataset of manually-annotated audio events that endeavors to bridge the gap in data availability between image and audio research. Using a carefully structured hierarchical ontology of 632 audio classes guided by the literature and manual curation, we collect data from human labelers to probe the presence of specific audio classes in 10 second segments of YouTube videos. Segments are proposed for labeling using searches based on metadata, context (e.g., links), and content analysis. The result is a dataset of unprecedented breadth and size that will, we hope, substantially stimulate the development of high-performance audio event recognizers.},
author = {Gemmeke, Jort F. and Ellis, Daniel P.W. and Freedman, Dylan and Jansen, Aren and Lawrence, Wade and Moore, R. Channing and Plakal, Manoj and Ritter, Marvin},
doi = {10.1109/ICASSP.2017.7952261},
file = {:home/lucas/work/mendeley/2017/Audio Set An Ontology and Human-Labeled Dataset for Audio Events{\_}Gemmeke et al.pdf:pdf},
isbn = {9781509041176},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Audio event detection,audio databases,data collection,sound ontology},
pages = {776--780},
title = {{Audio Set: An Ontology and Human-Labeled Dataset for Audio Events}},
year = {2017}
}
@inproceedings{10.1007/978-3-642-21916-0_75,
abstract = {This report presents an overview of the data mining contest organized in conjunction with the 19th International Symposium on Methodologies for Intelligent Systems (ISMIS 2011), in days between Jan 10 and Mar 21, 2011, on TunedIT competition platform. The contest consisted of two independent tasks, both related to music information retrieval: recognition of music genres and recognition of instruments, for a given music sample represented by a number of pre-extracted features. In this report, we describe aim of the contest, tasks formulation, procedures of data generation and parametrization, as well as final results of the competition.},
address = {Berlin, Heidelberg},
author = {Kostek, Bozena and Kupryjanow, Adam and Zwan, Pawel and Jiang, Wenxin and Ra{\'{s}}, Zbigniew W and Wojnarski, Marcin and Swietlicka, Joanna},
booktitle = {Foundations of Intelligent Systems},
editor = {Kryszkiewicz, Marzena and Rybinski, Henryk and Skowron, Andrzej and Ra{\'{s}}, Zbigniew W},
isbn = {978-3-642-21916-0},
pages = {715--724},
publisher = {Springer Berlin Heidelberg},
title = {{Report of the ISMIS 2011 Contest: Music Information Retrieval}},
year = {2011}
}
@article{Li2005,
author = {Li, Tao and Ogihara, Mitsunori},
file = {:home/lucas/work/mendeley/2005/Music Genre Classification with Taxonomy{\_}Li, Ogihara.pdf:pdf},
isbn = {0780388747},
journal = {In Proceedings of the 2005 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Philadelphia, PA, USA},
pages = {197--200},
publisher = {IEEE},
title = {{Music Genre Classification with Taxonomy}},
year = {2005}
}
@article{Schreiber2015,
abstract = {Any automatic music genre recognition (MGR) system must show its value in tests against a ground truth dataset. Recently, the public dataset most often used for this purpose has been proven problematic, because of mislabeling, duplications, and its relatively small size. Another dataset, the Million Song Dataset (MSD), a collection of features and metadata for one million tracks, unfortunately does not contain readily accessible genre labels. Therefore, multiple attempts have been made to add song-level genre annotations, which are required for supervised machine learning tasks. Thus far, the quality of these annotations has not been evaluated. In this paper we present a method for creating additional genre annotations for the MSD from databases, which contain multiple, crowd-sourced genre labels per song (Last.fm, beaTunes). Based on label co-occurrence rates, we derive taxonomies, which allow inference of top-level genres. These are most often used in MGR systems. We then combine multiple datasets using majority voting. This both promises a more reliable ground truth and allows the evaluation of the newly generated and preexisting datasets. To facilitate further research, all derived genre annotations are publicly available on our website.},
author = {Schreiber, Hendrik},
file = {:home/lucas/work/mendeley/2015/Improving Genre Annotations for the Million Song Dataset{\_}Schreiber.pdf:pdf},
isbn = {9788460688532},
journal = {Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015},
pages = {241--247},
title = {{Improving Genre Annotations for the Million Song Dataset}},
year = {2015}
}
@article{Lidy2005,
abstract = {We present a study on the importance of psycho-acoustic transformations for effective audio feature calculation. From the results, both crucial and problematic parts of the algorithm for Rhythm Patterns feature extraction are identified. We furthermore introduce two new feature representations in this context: Statistical Spectrum Descriptors and Rhythm Histogram features. Evaluation on both the individual and combined feature sets is accomplished through a music genre classification task, involving 3 reference audio collections. Results are compared to published measures on the same data sets. Experiments confirmed that in all settings the inclusion of psycho-acoustic transformations provides significant improvement of classification accuracy. {\textcopyright} 2005 Queen Mary, University of London.},
author = {Lidy, Thomas and Rauber, Andreas},
file = {:home/lucas/work/mendeley/2005/Evaluation of Feature Extractors and Psycho-Acoustic Transformations for Music Genre Classification{\_}Lidy, Rauber.pdf:pdf},
isbn = {9780955117909},
journal = {ISMIR 2005 - 6th International Conference on Music Information Retrieval},
keywords = {Audio feature extraction,Content-based retrieval,Music genre classification,Psycho-acoustic},
pages = {34--41},
title = {{Evaluation of Feature Extractors and Psycho-Acoustic Transformations for Music Genre Classification}},
year = {2005}
}
@book{Nurnberger2011,
abstract = {Retrieval of multimedia data is different from retrieval of structured data. A key problem in multimedia databases is search, and the proposed solutions to the problem of multimedia information retrieval span a rather wide spectrum of topics outside the traditional database area, ranging from information retrieval and humanâ€“computer interaction to computer vision and pattern recognition. Based on more than 10 years of teaching experience, Blanken and his coeditors have assembled all the topics that should be covered in advanced undergraduate or graduate courses on multimedia retrieval and multimedia databases. The single chapters of this textbook explain the general architecture of multimedia information retrieval systems; various metadata languages like Dublin Core, RDF, or MPEG; pattern recognition through Markov models, unsupervised learning, and pattern clustering; various indexing approaches to audio and video streams; interaction and control; the protection of content and user privacy; and search effectiveness and efficiency. The authors emphasize high-level features and show how these features are used in mathematical models to support the retrieval process. For each chapter, there's detail on further reading, and additional exercises and teaching material is available online.},
author = {Nurnberger, A and Detyniecki, Marcin},
booktitle = {Springer},
doi = {10.1007/b98244},
file = {:home/lucas/work/mendeley/2011/Adaptive Multimedia Retrieval{\_}Nurnberger, Detyniecki.pdf:pdf},
isbn = {9783642271687},
number = {July},
pages = {367},
title = {{Adaptive Multimedia Retrieval}},
url = {http://link.springer.com/content/pdf/10.1007/b98244.pdf},
year = {2011}
}
@article{Nanni2016,
abstract = {Since musical genre is one of the most common ways used by people for managing digital music databases, music genre recognition is a crucial task, deep studied by the Music Information Retrieval (MIR) research community since 2002. In this work we present a novel and effective approach for automated musical genre recognition based on the fusion of different set of features. Both acoustic and visual features are considered, evaluated, compared and fused in a final ensemble which show classification accuracy comparable or even better than other state-of-the-art approaches. The visual features are locally extracted from sub-windows of the spectrogram taken by Mel scale zoning: the input signal is represented by its spectrogram which is divided in sub-windows in order to extract local features; feature extraction is performed by calculating texture descriptors and bag of features projections from each sub-window; the final decision is taken using an ensemble of SVM classifiers. In this work we show for the first time that a bag of feature approach can be effective in this problem. As the acoustic features are concerned, we propose an ensemble of heterogeneous classifiers for maximizing the performance that could be obtained starting from the acoustic features. First timbre features are obtained from the audio signal, second some statistical measures are calculated from the texture window and the modulation spectrum, third a feature selection is executed to increase the recognition performance and decrease the computational complexity. Finally, the resulting descriptors are classified by fusing the scores of heterogeneous classifiers (SVM and Random subspace of AdaBoost). The experimental evaluation is performed on three well-known databases: the Latin Music Database (LMD), the ISMIR 2004 database and the GTZAN genre collection. The reported performance of the proposed approach is very encouraging, since they outperform other state-of-the-art approaches, without any ad hoc parameter optimization (i.e. using the same ensemble of classifiers and parameters setting in all the three datasets). The advantage of using both visual and audio features is also proved by means of Q-statistics, which confirms that the two sets of features are partially independent and they are suitable to be fused together in a heterogeneous system. The MATLAB code of the ensemble of classifiers and for the visual features extraction will be publicly available (see footnote 1) to other researchers for future comparisons. The code for acoustic features is not available since it is used in a commercial system.},
author = {Nanni, Loris and Costa, Yandre M.G. and Lumini, Alessandra and Kim, Moo Young and Baek, Seung Ryul},
doi = {10.1016/j.eswa.2015.09.018},
file = {:home/lucas/work/mendeley/2016/Combining Visual and Acoustic Features for Music Genre Classification{\_}Nanni et al.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Acoustic features,Ensemble of classifiers,Image processing,Music genre,Pattern recognition,Texture},
pages = {108--117},
publisher = {Elsevier Ltd},
title = {{Combining Visual and Acoustic Features for Music Genre Classification}},
url = {http://dx.doi.org/10.1016/j.eswa.2015.09.018},
volume = {45},
year = {2016}
}
@article{Li2006,
abstract = {Efficient and intelligent music information retrieval is a very important topic of the 21st century. With the ultimate goal of building personal music information retrieval systems, this paper studies the problem of intelligent music information retrieval. Huron [10] points out that since the preeminent functions of music are social and psychological, the most useful characterization would be based on four types of information: genre, emotion, style, and similarity. This paper introduces Daubechies Wavelet Coefficient Histograms (DWCH) for music feature extraction for music information retrieval. The histograms are computed from the coefficients of the dbg Daubechies wavelet filter applied to 3 s of music. A comparative study of sound features and classification algorithms on a dataset compiled by Tzanetakis shows that combining DWCH with timbrai features (MFCC and FFT), with the use of multiclass extensions of support vector machine, achieves approximately 80{\%} of accuracy, which is a significant improvement over the previously known result on this dataset. On another dataset the combination achieves 75{\%} of accuracy. The paper also studies the issue of detecting emotion in music. Rating of two subjects in the three bipolar adjective pairs are used. The accuracy of around 70 {\%} was achieved in predicting emotional labeling in these adjective pairs. The paper also studies the problem of identifying groups of artists based on their lyrics and sound using a semi-supervised classification algorithm. Identification of artist groups based on the Similar Artist lists at All Music Guide is attempted. The semi-supervised learning algorithm resulted in nontrivial increases in the accuracy to more than 70{\%}. Finally, the paper conducts a proof-of-concept experiment on similarity search using the feature set. {\textcopyright} 2006 IEEE.},
author = {Li, Tao and Ogihara, Mitsunori},
doi = {10.1109/TMM.2006.870730},
file = {:home/lucas/work/mendeley/2006/Toward Intelligent Music Information Retrieval{\_}Li, Ogihara.pdf:pdf},
issn = {15209210},
journal = {IEEE Transactions on Multimedia},
keywords = {Clustering,FFT,Machine learning,Music information retrieval,Wavelet},
number = {3},
pages = {564--574},
title = {{Toward Intelligent Music Information Retrieval}},
volume = {8},
year = {2006}
}
@article{Meng2016,
abstract = {Our system successfully determined the genre of the vast majority of the test songs. Not only did the system choose a genre, it quantified its output with a level of sureness.},
author = {Meng, Anders and Ahrendt, Peter and Larsen, Jan and Hansen, Lars Kai},
doi = {10.18535/ijecs/v4i10.38},
file = {:home/lucas/work/mendeley/2016/Temporal Feature Integration for Music Genre Classification{\_}Meng et al.pdf:pdf},
journal = {International Journal Of Engineering And Computer Science},
number = {5},
pages = {1654--1664},
title = {{Temporal Feature Integration for Music Genre Classification}},
volume = {15},
year = {2016}
}
@inproceedings{Defferrard2017,
abstract = {We introduce the Free Music Archive (FMA), an open and easily accessible dataset suitable for evaluating sev- eral tasks in MIR, a field concerned with browsing, search- ing, and organizing large music collections. The commu- nity's growing interest in feature and end-to-end learning is however restrained by the limited availability of large audio datasets. The FMA aims to overcome this hurdle by providing 917 GiB and 343 days of Creative Commons- licensed audio from 106,574 tracks from 16,341 artists and 14,854 albums, arranged in a hierarchical taxonomy of 161 genres. It provides full-length and high-quality audio, pre-computed features, together with track- and user-level metadata, tags, and free-form text such as biographies. We here describe the dataset and how it was created, propose a train/validation/test split and three subsets, discuss some suitable MIR tasks, and evaluate some baselines for genre recognition. Code, data, and usage examples are available at https://github.com/mdeff/fma.},
archivePrefix = {arXiv},
arxivId = {arXiv:1612.01840v3},
author = {Defferrard, Micha{\"{e}}l and Benzi, Kirell and Vandergheynst, Pierre and Bresson, Xavier},
booktitle = {18th International Society for Music Information Retrieval Conference (ISMIR)},
eprint = {arXiv:1612.01840v3},
file = {:home/lucas/work/mendeley/2017/FMA A Dataset for Music Analysis{\_}Defferrard et al.pdf:pdf},
title = {{FMA: A Dataset for Music Analysis}},
year = {2017}
}
@inproceedings{Tao,
abstract = {This paper proposes a novel content-based music genre classification method using temporal information and support vector machine. By processing of texture window statistics and delta computation, temporal information is incorporated to capture the time-varying behavior of music. 7 different temporal evolution descriptors applied in texture window are introduced in this paper, i.e., mean, standard deviation, maximum, minimum, temporal centroid, temporal skewness and temporal kurtosis. 4 different ways of representing a music clip are proposed and compared. The experimental results show that temporal information modeled by delta computation and texture window statistics is very effective and efficient in music genre classification. This paper uses a dataset consisting of 1000 30-sec music clips equally divided into 10 music genres, namely classical, blues, jazz, country, rock, metal, reggae, hiphop, disco and pop. The best classification accuracy achieved is 78.6{\%} for this 10-class music genre classification.},
author = {Tao, Ran and Li, Zhenyang and Ji, Ye},
booktitle = {ASCI Conference, 2010},
file = {:home/lucas/work/mendeley/2010/Music Genre Classification Using Temporal Information and Support Vector Machine{\_}Tao, Li, Ji.pdf:pdf},
keywords = {computing,music classification,music content analysis},
title = {{Music Genre Classification Using Temporal Information and Support Vector Machine}},
year = {2010}
}
@incollection{Sturm2014,
author = {Sturm, Bob L.},
booktitle = {Adaptive Multimedia Retrieval: Semantics, Context, and Adaptation},
doi = {10.1007/978-3-319-12093-5_2},
file = {:home/lucas/work/mendeley/2011/Adaptive Multimedia Retrieval{\_}Nurnberger, Detyniecki.pdf:pdf},
isbn = {978-3-319-12093-5},
pages = {29--66},
title = {{A Survey of Evaluation in Music Genre Recognition}},
url = {http://link.springer.com/10.1007/978-3-319-12093-5{\_}2},
year = {2014}
}
@article{Kotropoulos2010,
abstract = {Resorting to the rich, psycho-physiologically grounded, properties of the slow temporal modulations of music recordings, a novel classifier ensemble is built, which applies discriminant sparse projections. More specifically, overcomplete dictionaries are learned and sparse coefficient vectors are extracted to optimally approximate the slow temporal modulations of the training music recordings. The sparse coefficient vectors are then projected to the principal subspaces of their within-class and between-class covariance matrices. Decisions are taken with respect to the minimum Euclidean distance from the class mean sparse coefficient vectors, which undergo the aforementioned projections. The application of majority voting to the decisions taken by 10 individual classifiers, which are trained on the 10 training folds defined by stratified 10-fold cross-validation on the GTZAN dataset, yields a music genre classification accuracy of 84.96{\%} on average. The latter exceeds by 2.46{\%} the highest accuracy previously reported without employing any sparse representations. {\textcopyright} 2010 IEEE.},
author = {Kotropoulos, Constantine and Arce, Gonzalo R. and Panagakis, Yannis},
doi = {10.1109/ICPR.2010.207},
file = {:home/lucas/work/mendeley/2010/Ensemble Discriminant Sparse Projections Applied to Music Genre Classification{\_}Kotropoulos, Arce, Panagakis.pdf:pdf},
isbn = {9780769541099},
issn = {10514651},
journal = {Proceedings - International Conference on Pattern Recognition},
pages = {822--825},
publisher = {IEEE},
title = {{Ensemble Discriminant Sparse Projections Applied to Music Genre Classification}},
year = {2010}
}
@article{RiyoichiSawadaUeno2019,
abstract = {Automatic music organization and retrieval is a highly required task nowadays. Labeling songs with summarized but descriptive information have implications in a wide range of tasks in this scenario. The genre is one of the most common labels used for music recordings. Using this piece of information, music platforms can organize collections by, for instance, associating songs and artists with similar characteristics. Lyrics represent an alternative source of data for genre recognition. While 'traditional' bag-of-words-based text mining techniques represent a considerable part of the literature, recent papers shown an advance on this task applying deep learning algorithms. However, there is no research on how these distinct strategies contribute to each other. In this paper, we explore different strategies for music genre classification from lyrics and show that even simple combinations of these strategies allow improving accuracy on the lyrics-based music genre identification.},
author = {{Riyoichi Sawada Ueno}, Caio Luiggy and {Furtado Silva}, Diego},
doi = {10.1109/BRACIS.2019.00033},
file = {:home/lucas/work/mendeley/2019/On Combining Diverse Models for Lyrics-Based Music Genre Classification{\_}Riyoichi Sawada Ueno, Furtado Silva.pdf:pdf},
isbn = {9781728142531},
journal = {Proceedings - 2019 Brazilian Conference on Intelligent Systems, BRACIS 2019},
keywords = {Ensemble,Machine learning,Music information retrieval,Text mining},
pages = {138--143},
publisher = {IEEE},
title = {{On Combining Diverse Models for Lyrics-Based Music Genre Classification}},
year = {2019}
}
@article{Bahuleyan2018,
abstract = {Categorizing music files according to their genre is a challenging task in the area of music information retrieval (MIR). In this study, we compare the performance of two classes of models. The first is a deep learning approach wherein a CNN model is trained end-to-end, to predict the genre label of an audio signal, solely using its spectrogram. The second approach utilizes hand-crafted features, both from the time domain and the frequency domain. We train four traditional machine learning classifiers with these features and compare their performance. The features that contribute the most towards this multi-class classification task are identified. The experiments are conducted on the Audio set data set and we report an AUC value of 0.894 for an ensemble classifier which combines the two proposed approaches.},
archivePrefix = {arXiv},
arxivId = {arXiv:1804.01149v1},
author = {Bahuleyan, Hareesh},
doi = {10.1109/siu.2017.7960694},
eprint = {arXiv:1804.01149v1},
file = {:home/lucas/work/mendeley/2018/Music Genre Classification with Machine Learning Techniques{\_}Bahuleyan.pdf:pdf},
pages = {1--4},
title = {{Music Genre Classification with Machine Learning Techniques}},
year = {2018}
}
@article{Lim2012,
abstract = {An automatic classification system of the music genres is proposed. Based on the timbre features such as melfrequency cepstral coefficients, the spectro-temporal features are obtained to capture the temporal evolution and variation of the spectral characteristics of the music signal. Mean, variance, minimum, and maximum values of the timbre features are calculated. Modulation spectral flatness, crest, contrast, and valley are estimated for both original spectra and timbre-feature vectors. A support vector machine (SVM) is used as a classifier where an elaborated kernel function is defined. To reduce the computational complexity, an SVM ranker is applied for feature selection. Compared with the best algorithms submitted to the music information retrieval evaluation exchange (MIREX) contests, the proposed method provides higher accuracy at a lower feature dimension for the GTZAN and ISMIR2004 databases. {\textcopyright} 2011 IEEE.},
author = {Lim, Shin Cheol and Lee, Jong Seol and Jang, Sei Jin and Lee, Soek Pil and Kim, Moo Young},
doi = {10.1109/TCE.2012.6414994},
file = {:home/lucas/work/mendeley/2012/Music-Genre Classification System Based on Spectro-Temporal Features and Feature Selection{\_}Lim et al.pdf:pdf},
issn = {00983063},
journal = {IEEE Transactions on Consumer Electronics},
keywords = {Music genre classification,SVM,feature selection,modulation spectrum,music informationretrieval},
number = {4},
pages = {1262--1268},
publisher = {IEEE},
title = {{Music-Genre Classification System Based on Spectro-Temporal Features and Feature Selection}},
volume = {58},
year = {2012}
}
@inproceedings{Harte2006,
abstract = {We propose a novel method for detecting changes in the harmonic content of musical audio signals. Our method uses a new model for Equal Tempered Pitch Class Space. This model maps 12-bin chroma vectors to the interior space of a 6-D polytope; pitch classes are mapped onto the vertices of this polytope. Close harmonic relations such as fifths and thirds appear as small Euclidian distances. We calculate the Euclidian distance between analysis frames n +1 and n -1 to develop a harmonic change measure for frame n. A peak in the detection function denotes a transition from one harmonically stable region to another. Initial experiments show that the algorithm can successfully detect harmonic changes such as chord boundaries in polyphonic audio recordings. {\textcopyright} 2006 ACM.},
address = {New York, New York, USA},
author = {Harte, Christopher and Sandler, Mark and Gasser, Martin},
booktitle = {Proceedings of the 1st ACM workshop on Audio and music computing multimedia - AMCMM '06},
doi = {10.1145/1178723.1178727},
file = {:home/lucas/work/mendeley/2006/Detecting harmonic change in musical audio{\_}Harte, Sandler, Gasser.pdf:pdf},
isbn = {1595935010},
keywords = {Audio,Harmonic,Music,Pitch space,Segmentation},
pages = {21},
publisher = {ACM Press},
title = {{Detecting harmonic change in musical audio}},
url = {http://portal.acm.org/citation.cfm?doid=1178723.1178727},
year = {2006}
}
@article{Sturm2013,
abstract = {The GTZAN dataset appears in at least 100 published works, and is the most-used public dataset for evaluation in machine listening research for music genre recognition (MGR). Our recent work, however, shows GTZAN has several faults (repetitions, mislabelings, and distortions), which challenge the interpretability of any result derived using it. In this article, we disprove the claims that all MGR systems are affected in the same ways by these faults, and that the performances of MGR systems in GTZAN are still meaningfully comparable since they all face the same faults. We identify and analyze the contents of GTZAN, and provide a catalog of its faults. We review how GTZAN has been used in MGR research, and find few indications that its faults have been known and considered. Finally, we rigorously study the effects of its faults on evaluating five different MGR systems. The lesson is not to banish GTZAN, but to use it with consideration of its contents.},
archivePrefix = {arXiv},
arxivId = {1306.1461},
author = {Sturm, Bob L.},
doi = {10.1080/09298215.2014.894533},
eprint = {1306.1461},
file = {:home/lucas/work/mendeley/2013/The GTZAN Dataset Its Contents, Its Faults, Their Effects on Evaluation, and Its Future Use{\_}Sturm.pdf:pdf},
number = {11},
pages = {1--29},
title = {{The GTZAN Dataset: Its Contents, Its Faults, Their Effects on Evaluation, and Its Future Use}},
url = {http://arxiv.org/abs/1306.1461{\%}0Ahttp://dx.doi.org/10.1080/09298215.2014.894533},
year = {2013}
}
@article{Bertin-Mahieux2011,
abstract = {We introduce the Million Song Dataset, a freely-available collection of audio features and metadata for a million contemporary popular music tracks. We describe its creation process, its content, and its possible uses. Attractive features of the Million Song Database include the range of existing resources to which it is linked, and the fact that it is the largest current research dataset in our field. As an illustration, we present year prediction as an example application, a task that has, until now, been difficult to study owing to the absence of a large set of suitable data. We show positive results on year prediction, and discuss more generally the future development of the dataset. {\textcopyright} 2011 International Society for Music Information Retrieval.},
author = {Bertin-Mahieux, Thierry and Ellis, Daniel P.W. and Whitman, Brian and Lamere, Paul},
file = {:home/lucas/work/mendeley/2011/The Million Song Dataset{\_}Bertin-Mahieux et al.pdf:pdf},
isbn = {9780615548654},
journal = {Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011},
pages = {591--596},
title = {{The Million Song Dataset}},
year = {2011}
}
@article{Zhang2016,
abstract = {In recent years, deep neural networks have been shown to be effective in many classification tasks, including music genre classification. In this paper, we proposed two ways to improve music genre classification with convolutional neural networks: 1) combining max-and averagepooling to provide more statistical information to higher level neural networks; 2) using shortcut connections to skip one or more layers, a method inspired by residual learning method. The input of the CNN is simply the short time Fourier transforms of the audio signal. The output of the CNN is fed into another deep neural network to do classification. By comparing two different network topologies, our preliminary experimental results on the GTZAN data set show that the above two methods can effectively improve the classification accuracy, especially the second one.},
author = {Zhang, Weibin and Lei, Wenkang and Xu, Xiangmin and Xing, Xiaofeng},
doi = {10.21437/Interspeech.2016-1236},
file = {:home/lucas/work/mendeley/2016/Improved Music Genre Classification with Convolutional Neural Networks{\_}Zhang et al.pdf:pdf},
issn = {19909772},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
keywords = {Convolutional neural network,Music genre classification,Residual learning},
pages = {3304--3308},
title = {{Improved Music Genre Classification with Convolutional Neural Networks}},
volume = {08-12-Sept},
year = {2016}
}
@inproceedings{Defferrard2018,
address = {New York, New York, USA},
archivePrefix = {arXiv},
arxivId = {arXiv:1803.05337v1},
author = {Defferrard, Micha{\"{e}}l and Mohanty, Sharada P. and Carroll, Sean F. and Salath{\'{e}}, Marcel},
booktitle = {Companion of the The Web Conference 2018 on The Web Conference 2018 - WWW '18},
doi = {10.1145/3184558.3192310},
eprint = {arXiv:1803.05337v1},
file = {:home/lucas/work/mendeley/2018/Learning to Recognize Musical Genre from Audio{\_}Defferrard et al.pdf:pdf},
isbn = {9781450356404},
keywords = {acm reference format,and marcel salath{\'{e}},carroll,micha{\"{e}}l defferrard,mir,ml challenge,mohanty,music information retrieval,open data,sean f,sharada p},
number = {1},
pages = {1921--1922},
publisher = {ACM Press},
title = {{Learning to Recognize Musical Genre from Audio}},
url = {http://dl.acm.org/citation.cfm?doid=3184558.3192310},
year = {2018}
}
@article{McFee2015,
abstract = {â€”This document describes version 0.4.0 of librosa: a Python pack-age for audio and music signal processing. At a high level, librosa provides implementations of a variety of common functions used throughout the field of music information retrieval. In this document, a brief overview of the library's functionality is provided, along with explanations of the design goals, software development practices, and notational conventions.},
author = {McFee, Brian and Raffel, Colin and Liang, Dawen and Ellis, Daniel and McVicar, Matt and Battenberg, Eric and Nieto, Oriol},
doi = {10.25080/majora-7b98e3ed-003},
file = {:home/lucas/work/mendeley/2015/librosa Audio and Music Signal Analysis in Python{\_}McFee et al.pdf:pdf},
journal = {Proceedings of the 14th Python in Science Conference},
number = {Scipy},
pages = {18--24},
title = {{librosa: Audio and Music Signal Analysis in Python}},
year = {2015}
}
