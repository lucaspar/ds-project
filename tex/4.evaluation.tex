\section{Evaluation}

In order to compare the classification models and fulfill our contributions of (i) which features are most relevant in the handcrafted method, and (ii) build an ensemble model for music genre classification; we plan to extract a list of metrics from our classifiers. Firstly, our dataset will be split into training, validation, and testing sets with disjoint audio tracks and uniform representation across music genres, when possible. Then, once the classifiers models are built, we will extract the metrics below in isolation, and lastly, from our ensemble version.

\subsection{Metrics}

Some of the metrics we are considering using are:

\begin{itemize}
    \item Mean accuracy;
    \item F1 score;
    \item Confusion matrix;
    \item Area under the ROC Curve (AUC); % if any thresholding is applied
    % Q:how to determine which features are most relevant?
\end{itemize}
